version: '3.8'
services:
  llama_server:
    build: .
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    volumes:
      - /home/kdlocpanda/personal/my_repos/josh_bot/llamafiles/:/llamafiles # Replace with your local path to Llamafiles
      - /home/kdlocpanda/personal/models:/model_dir # Replace with your local path to model directory
    command: ["./model_dir/mistral-7b-instruct-v0.1-Q4_K_M-server.llamafile", "--ngl", "5"]
    # Replace "user:user" with the appropriate user and group IDs
    # The model file is used as an example, replace with the actual one you need to execute
    environment:
      - MODEL_DIR=/model_dir
    networks:
      - llama_network
    ports:
      - "8080:8080" # Expose port 8080 of the container by mapping it to port 8080 on the host machine

networks:
  llama_network:
    driver: bridge