{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install unstructured[\"pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "\n",
    "\n",
    "loader = DirectoryLoader(\"test_loader\",glob=\"*.pdf\",show_progress=True)\n",
    "docs = loader.load()\n",
    "\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"test_loader\",glob=\"*.pdf\")\n",
    "data = loader.load()\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ocean, vast and eternal, covers over 70% of our planet and yet remains largely unknown to us. It harbors mysteries within its depths that we have scarcely begun to understand; it houses entire ecosystems beyond our wildest imagination; and it is both a source of life and death for countless creatures that call it home. Its tides ebb and flow like the rhythmic beating of a living heart, reminding us that even in the face of boundless power and unlimited expanse, there exists an interconnectedness between all things in this world. To truly understand the ocean is to grasp but a fraction of its infinite complexity; to seek knowledge of it is to embrace uncertainty and accept that our small place within it is just one speck in a never-ending cosmos. Yet despite the daunting scale of our task, let us endeavor to learn all we can about this enigmatic blue world; for with each discovery, we gain insight into not only its wondrous secrets but also those that reside deep within ourselves.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY']=\"sk-111111111111111111111111111111111111111111111111\"\n",
    "os.environ['OPENAI_API_BASE']=\"http://127.0.0.1:5001/v1\"\n",
    "import openai\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"x\",\n",
    "  messages = [{ 'role': 'system', 'content': \"Answer in a consistent style.\" },\n",
    "    {'role': 'user', 'content': \"Teach me about patience.\"},\n",
    "    {'role': 'assistant', 'content': \"The river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\"},\n",
    "    {'role': 'user', 'content': \"Teach me about the ocean.\"},\n",
    "  ]\n",
    ")\n",
    "text = response['choices'][0]['message']['content']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "print(fibonacci(8)) # prints 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(\"VerastreamModeltutorial.pdf\",extract_images=True)\n",
    "data = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytube -q\n",
    "!pip install youtube_transcript_api -q\n",
    "!pip install amazon-textract-caller -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=-1sdWLr3TbI\", add_video_info=True\n",
    ")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain.document_loaders import AmazonTextractPDFLoader\n",
    "\n",
    "\n",
    "textract_client = boto3.client('textract')\n",
    "\n",
    "file_path = \"s3://s3-special-test-jy/VerastreamModeltutorial.pdf\"\n",
    "loader = AmazonTextractPDFLoader(file_path, client=textract_client)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AmazonTextractImageLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "from langchain.embeddings import GPT4AllEmbeddings   \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(\"VerastreamModeltutorial.pdf\", extract_images=True)\n",
    "data = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "print(all_splits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#vectorstore = Chroma.from_documents(persist_directory=documents=all_splits, embedding=GPT4AllEmbeddings())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "df = pd.json_normalize(json.loads(pd.DataFrame(data).to_json(orient=\"records\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Team</th>\n",
    "      <th>\"Payroll (millions)\"</th>\n",
    "      <th>\"Wins\"</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Nationals</td>\n",
    "      <td>81.34</td>\n",
    "      <td>98</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Reds</td>\n",
    "      <td>82.20</td>\n",
    "      <td>97</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Yankees</td>\n",
    "      <td>197.96</td>\n",
    "      <td>95</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>Giants</td>\n",
    "      <td>117.62</td>\n",
    "      <td>94</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>Braves</td>\n",
    "      <td>83.31</td>\n",
    "      <td>94</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from langchain.chat_models import ChatOpenAI, ChatOllama\n",
    "from langchain.chains import create_extraction_chain\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "from langchain.document_transformers import BeautifulSoupTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings.gpt4all import GPT4AllEmbeddings\n",
    "\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self, model_name=\"gpt-3.5-turbo-0613\", ollama_model=None):\n",
    "        if ollama_model:\n",
    "            self.llm = ChatOllama(\n",
    "                model=ollama_model,\n",
    "                callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "            )\n",
    "        else:\n",
    "            self.llm = ChatOpenAI(model=model_name)\n",
    "\n",
    "    def extract(self, content: str, schema: dict):\n",
    "        return create_extraction_chain(schema=schema, llm=self.llm).run(content)\n",
    "\n",
    "    def scrape_with_playwright(self,urls, schema):\n",
    "        loader = AsyncChromiumLoader(urls)\n",
    "        docs = loader.load()\n",
    "        bs_transformer = BeautifulSoupTransformer()\n",
    "        docs_transformed = bs_transformer.transform_documents(\n",
    "            docs, tags_to_extract=[\"span\"]\n",
    "        )\n",
    "        print(f\"Number of documents: {len(docs_transformed)}\")\n",
    "        print(docs_transformed[0].page_content)\n",
    "        # Grab the first 1000 tokens of the site\n",
    "        splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=1000, chunk_overlap=0\n",
    "        )\n",
    "        splits = splitter.split_documents(docs_transformed)\n",
    "        vectorstore = Chroma.from_documents(persist_directory='testing_wsj',documents=splits, embedding=GPT4AllEmbeddings())\n",
    "\n",
    "       \n",
    "        # Process the first split\n",
    "        extracted_content = self.extract(schema=schema, content=splits[0].page_content)\n",
    "        pprint.pprint(extracted_content)\n",
    "        return extracted_content\n",
    "\n",
    "    def run_scraper(self, urls, schema):\n",
    "        return self.scrape_with_playwright(urls, schema)\n",
    "\n",
    "# Usage:\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"news_article_title\": {\"type\": \"string\"},\n",
    "        \"news_article_summary\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"news_article_title\", \"news_article_summary\"],\n",
    "}\n",
    "\n",
    "# Usage with ChatOpenAI:\n",
    "#web_scraper_openai = WebScraper()\n",
    "#extracted_content_openai = web_scraper_openai.run_scraper([\"https://www.wsj.com\"], schema)\n",
    "\n",
    "# Usage with ChatOllama:\n",
    "web_scraper_ollama = WebScraper(ollama_model=\"mistral:7b\")\n",
    "extracted_content_ollama = web_scraper_ollama.run_scraper([\"https://www.wsj.com\"], schema)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
