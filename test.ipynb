{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all-minilm:latest',\n",
       " 'codellama:latest',\n",
       " 'llama2:latest',\n",
       " 'llava:latest',\n",
       " 'mistral:instruct',\n",
       " 'mistral:latest',\n",
       " 'mistral-openorca:latest',\n",
       " 'orca-mini:latest',\n",
       " 'stablelm2:latest',\n",
       " 'zephyr:latest']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"http://192.168.86.29:11434\"\n",
    "\n",
    "def list_models():\n",
    "    response = requests.get(f\"{BASE_URL}/api/tags\")\n",
    "    if response.status_code == 200:\n",
    "        models = response.json().get(\"models\", [])\n",
    "        return [model[\"name\"] for model in models]\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from rich import print as rprint\n",
    "from prompts import presidents\n",
    "import requests\n",
    "from prompt_toolkit import prompt\n",
    "from prompt_toolkit.completion import WordCompleter\n",
    "\n",
    "\n",
    "BASE_URL = \"http://192.168.86.29:11434\"\n",
    "\n",
    "def list_models():\n",
    "    response = requests.get(f\"{BASE_URL}/api/tags\")\n",
    "    if response.status_code == 200:\n",
    "        models = response.json().get(\"models\", [])\n",
    "        return [model[\"name\"] for model in models]\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "class Term(BaseModel):\n",
    "    start: str = Field(..., description=\"The start year of the president's term.\")\n",
    "    end: str = Field(..., description=\"The end year of the president's term .\")\n",
    "\n",
    "\n",
    "class President(BaseModel):\n",
    "    name: str = Field(..., description=\"The name of the president.\")\n",
    "    birth_year: int = Field(..., description=\"The birth year of the president.\")\n",
    "    death_year: int = Field(..., description=\"The death year of the president.\")\n",
    "    party: str = Field(..., description=\"The political party of the president.\")\n",
    "    term: Term = Field(..., description=\"The term of the president.\")\n",
    "    interesting_fact: str = Field(..., description=\"An interesting fact about the president.\")\n",
    "    popular_vote: int = Field(..., description=\"The popular vote of the president.\")\n",
    "\n",
    "\n",
    "\n",
    "def get_presidents(model_name: str):\n",
    "    model = ChatOllama(model=model_name ,base_url=BASE_URL)\n",
    "    # query a random president from presidents list\n",
    "    query = pd.DataFrame(presidents).sample(1).to_string(index=False, header=False)\n",
    "    rprint(f\"US PRESIDENT CHOSEN: {query}\")\n",
    "\n",
    "    parser = JsonOutputParser(pydantic_object=President)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "        input_variables=[\"query\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "\n",
    "    chain = prompt | model | parser\n",
    "\n",
    "\n",
    "    output = chain.invoke({\"query\": query})\n",
    "\n",
    "\n",
    "    rprint(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "#model_name_list = pd.DataFrame(list_models())\n",
    "## need to remove all-minilm:latest from the dataframe\n",
    "#model_name_list = model_name_list[model_name_list[0] != \"all-minilm:latest\"][0].to_list()\n",
    "model_name_list = [    'dolphin-mistral:latest',\n",
    "    'dolphin-phi:latest',\n",
    "    'gemma:7b',\n",
    "    'gemma:instruct',\n",
    "    'llama2:latest',\n",
    "    'llava:latest',\n",
    "    'mistral:instruct',\n",
    "    'mistral:latest',\n",
    "    'stablelm2:latest',\n",
    "    'tinydolphin:latest',\n",
    "    'zephyr:latest'\n",
    "]\n",
    "model_completer = WordCompleter(model_name_list, ignore_case=True)\n",
    "\n",
    "model_name = prompt(\"Select a model: \", completer=model_completer)\n",
    "\n",
    "rprint(f\"TESTING: {model_name}\")\n",
    "try:\n",
    "    get_presidents(model_name)\n",
    "    rprint(\"\\n\\n\\n\")\n",
    "except Exception as e:\n",
    "    rprint(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  2.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "\n",
    "urls = [\"https://python.langchain.com/docs/\"]\n",
    "loader = AsyncHtmlLoader(urls)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='<!doctype html>\\n<html lang=\"en\" dir=\"ltr\" data-has-hydrated=\"false\">\\n<head>\\n<meta charset=\"UTF-8\">\\n<meta name=\"generator\" content=\"Docusaurus v2.4.3\">\\n<title data-rh=\"true\">ü¶úÔ∏èüîó Langchain</title><meta data-rh=\"true\" property=\"og:title\" content=\"ü¶úÔ∏èüîó Langchain\"><meta data-rh=\"true\" name=\"viewport\" content=\"width=device-width,initial-scale=1\"><meta data-rh=\"true\" name=\"twitter:card\" content=\"summary_large_image\"><meta data-rh=\"true\" property=\"og:image\" content=\"https://python.langchain.com/img/brand/theme-image.png\"><meta data-rh=\"true\" name=\"twitter:image\" content=\"https://python.langchain.com/img/brand/theme-image.png\"><meta data-rh=\"true\" property=\"og:url\" content=\"https://python.langchain.com/\"><meta data-rh=\"true\" name=\"docusaurus_locale\" content=\"en\"><meta data-rh=\"true\" name=\"docusaurus_tag\" content=\"default\"><meta data-rh=\"true\" name=\"docsearch:language\" content=\"en\"><meta data-rh=\"true\" name=\"docsearch:docusaurus_tag\" content=\"default\"><link data-rh=\"true\" rel=\"icon\" href=\"/img/brand/favicon.png\"><link data-rh=\"true\" rel=\"canonical\" href=\"https://python.langchain.com/\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://python.langchain.com/\" hreflang=\"en\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://python.langchain.com/\" hreflang=\"x-default\"><script data-rh=\"true\">function maybeInsertBanner(){window.__DOCUSAURUS_INSERT_BASEURL_BANNER&&insertBanner()}function insertBanner(){var n=document.getElementById(\"__docusaurus-base-url-issue-banner-container\");if(n){n.innerHTML=\\'\\\\n<div id=\"__docusaurus-base-url-issue-banner\" style=\"border: thick solid red; background-color: rgb(255, 230, 179); margin: 20px; padding: 20px; font-size: 20px;\">\\\\n   <p style=\"font-weight: bold; font-size: 30px;\">Your Docusaurus site did not load properly.</p>\\\\n   <p>A very common reason is a wrong site <a href=\"https://docusaurus.io/docs/docusaurus.config.js/#baseUrl\" style=\"font-weight: bold;\">baseUrl configuration</a>.</p>\\\\n   <p>Current configured baseUrl = <span style=\"font-weight: bold; color: red;\">/</span>  (default value)</p>\\\\n   <p>We suggest trying baseUrl = <span id=\"__docusaurus-base-url-issue-banner-suggestion-container\" style=\"font-weight: bold; color: green;\"></span></p>\\\\n</div>\\\\n\\';var e=document.getElementById(\"__docusaurus-base-url-issue-banner-suggestion-container\"),s=window.location.pathname,r=\"/\"===s.substr(-1)?s:s+\"/\";e.innerHTML=r}}window.__DOCUSAURUS_INSERT_BASEURL_BANNER=!0,document.addEventListener(\"DOMContentLoaded\",maybeInsertBanner)</script><link rel=\"search\" type=\"application/opensearchdescription+xml\" title=\"ü¶úÔ∏èüîó Langchain\" href=\"/opensearch.xml\">\\n\\n\\n<script src=\"/js/google_analytics.js\"></script>\\n<script src=\"https://www.googletagmanager.com/gtag/js?id=G-9B66JQQH2F\" async></script><link rel=\"stylesheet\" href=\"/assets/css/styles.48c66593.css\">\\n<link rel=\"preload\" href=\"/assets/js/runtime~main.da0daa55.js\" as=\"script\">\\n<link rel=\"preload\" href=\"/assets/js/main.8e567e9f.js\" as=\"script\">\\n</head>\\n<body class=\"navigation-with-keyboard\">\\n<script>!function(){function e(e){document.documentElement.setAttribute(\"data-theme\",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get(\"docusaurus-theme\")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem(\"theme\")}catch(e){}return e}();null!==t?e(t):window.matchMedia(\"(prefers-color-scheme: dark)\").matches?e(\"dark\"):(window.matchMedia(\"(prefers-color-scheme: light)\").matches,e(\"light\"))}()</script><div id=\"__docusaurus\">\\n<div id=\"__docusaurus-base-url-issue-banner-container\"></div></div>\\n<script src=\"/assets/js/runtime~main.da0daa55.js\"></script>\\n<script src=\"/assets/js/main.8e567e9f.js\"></script>\\n</body>\\n</html>', metadata={'source': 'https://python.langchain.com/docs/', 'title': 'ü¶úÔ∏èüîó Langchain', 'language': 'en'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(model='LLaMA_CPP',\n",
    "                    temperature=0,\n",
    "                    openai_api_key=\"sk-no-key-required\", \n",
    "                    base_url='http://localhost:8080/v1',\n",
    "                    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "\n",
    "template = \"\"\"turn the following user input into a search query for a search engine:\n",
    "\n",
    "{input}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(base_url='http://localhost:8080/v1',\n",
    "                   model='LLaMA_CPP')\n",
    "\n",
    "chain = prompt | model | StrOutputParser() | search\n",
    "\n",
    "\n",
    "chain.invoke({\"input\": \"Current news openai\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI(model=\"LLaMA_CPP\",\n",
    "                    openai_api_key=\"sk-no-key-required\", \n",
    "                    base_url='http://localhost:8080/v1')\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "print(chain)\n",
    "\n",
    "\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rich --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "# \n",
    "def search(query):\n",
    "    search = DuckDuckGoSearchRun()\n",
    "    return search.invoke({\"query\": query})\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def search_results():\n",
    "    search = DuckDuckGoSearchResults()\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "search = DuckDuckGoSearchResults()\n",
    "item = search.run(\"Obama's first name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-0125\",\n",
    "  response_format={ \"type\": \"json_object\" },\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "\n",
    "    # required but ignored\n",
    "    api_key='ollama',\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    response_format={ \"type\": \"json_object\" },\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Provide me a list of 5 top tech companies that are listed on US stock exchanges. Include their CEO's name and stock ticker and one interesting fact about each company.\"}\n",
    "    ],\n",
    "    model='no-censor-dolph-mistral:latest',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_object = json.loads(json_string)\n",
    "json_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.json_normalize(json_object['companies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasai import SmartDataframe\n",
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "langchain_llm = Ollama(model=\"mistral\")\n",
    "df = SmartDataframe(df, config={\"llm\": langchain_llm})\n",
    "df.chat('who is the ceo of fb based on the data?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasai import SmartDataframe\n",
    "import pandas as pd\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "\n",
    "langchain_llm = Ollama(model=\"mistral\")\n",
    "\n",
    "df = SmartDataframe(df, config={\"llm\": langchain_llm})\n",
    "df.chat('Tell me about this dataset?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.chat('And what is the GDP of each of these countries?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
